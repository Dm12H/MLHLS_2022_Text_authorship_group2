{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d81b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125ac726",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFRAME_PATH=\"/home/dm12h/authorship/temp/aggregated_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3eeded4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DFRAME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f0b942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_punkt</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аверченко</td>\n",
       "      <td>Averchenko_A-T-Averchenko-Sobranie-sochineniy-...</td>\n",
       "      <td>Мы за пять лет. Материалы [к биографии]\\nКак б...</td>\n",
       "      <td>Мы за пять лет Материалы к биографии Как будто...</td>\n",
       "      <td>пять год материал биография кроваво-красный ра...</td>\n",
       "      <td>4_NUMR 3_NOUN 9_NOUN 9_NOUN 15_ADJF 6_NOUN 8_V...</td>\n",
       "      <td>Мы за пять лет . Материалы [ к биографии ] Как...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аверченко</td>\n",
       "      <td>Averchenko_A-T-Averchenko-Sobranie-sochineniy-...</td>\n",
       "      <td>Сидел он за большим письменным столом перед де...</td>\n",
       "      <td>Сидел он за большим письменным столом перед де...</td>\n",
       "      <td>сидеть больший письменный стол деревянный доск...</td>\n",
       "      <td>5_VERB 7_ADJF 10_ADJF 6_NOUN 10_ADJF 6_NOUN 6_...</td>\n",
       "      <td>Сидел он за большим письменным столом перед де...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Аверченко</td>\n",
       "      <td>Averchenko_A-T-Averchenko-Sobranie-sochineniy-...</td>\n",
       "      <td>— Да уж, — качал головой сдержанный Ре-ми. — Н...</td>\n",
       "      <td>Да уж качал головой сдержанный Ре-ми Нехорошо ...</td>\n",
       "      <td>качать голова сдержать ре-ми нехорошо нехорошо...</td>\n",
       "      <td>5_VERB 7_NOUN 10_PRTF 5_None 8_ADVB 8_ADVB 4_A...</td>\n",
       "      <td>— Да уж , — качал головой сдержанный Ре-ми . —...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Аверченко</td>\n",
       "      <td>Averchenko_A-T-Averchenko-Sobranie-sochineniy-...</td>\n",
       "      <td>Поверит ли кто-нибудь, что нами за эти пять ле...</td>\n",
       "      <td>Поверит ли кто-нибудь что нами за эти пять лет...</td>\n",
       "      <td>поверить кто-нибудь пять год совместно м.г кор...</td>\n",
       "      <td>7_VERB 10_NPRO 4_NUMR 3_NOUN 9_ADVB 3_None 11_...</td>\n",
       "      <td>Поверит ли кто-нибудь , что нами за эти пять л...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Аверченко</td>\n",
       "      <td>Averchenko_A-T-Averchenko-Sobranie-sochineniy-...</td>\n",
       "      <td>8\\nАверченко А. Избранные рассказы. М., 1985. ...</td>\n",
       "      <td>8 Аверченко А Избранные рассказы М 1985 С 7 9 ...</td>\n",
       "      <td>8 избранный рассказ м 1985 7 9 ежегодник 156 1...</td>\n",
       "      <td>1_None 9_ADJF 8_NOUN 1_NOUN 4_None 1_None 1_No...</td>\n",
       "      <td>8 Аверченко А. Избранные рассказы . М. , 1985 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17731</th>\n",
       "      <td>Чехов</td>\n",
       "      <td>zhivoi-tovar.epub</td>\n",
       "      <td>— Это ужасно, Григорий Васильич! — заговорил Б...</td>\n",
       "      <td>Это ужасно Григорий Васильич заговорил Бугров ...</td>\n",
       "      <td>это ужасно григорий васильич заговорить бугров...</td>\n",
       "      <td>3_PRCL 6_ADVB 8_NOUN 8_NOUN 9_VERB 6_NOUN 6_GR...</td>\n",
       "      <td>— Это ужасно , Григорий Васильич ! — заговорил...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17732</th>\n",
       "      <td>Чехов</td>\n",
       "      <td>zhivoi-tovar.epub</td>\n",
       "      <td>Грохольский жил всё в той же даче… Надежды и ж...</td>\n",
       "      <td>Грохольский жил всё в той же даче Надежды и же...</td>\n",
       "      <td>грохольский жить всё дача надежда желание мале...</td>\n",
       "      <td>11_ADJF 3_VERB 3_PRCL 4_NOUN 7_NOUN 7_NOUN 9_A...</td>\n",
       "      <td>Грохольский жил всё в той же даче… Надежды и ж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17733</th>\n",
       "      <td>Чехов</td>\n",
       "      <td>zhivoi-tovar.epub</td>\n",
       "      <td>Бугров держал Лизу за талию и говорил ей:\\n— М...</td>\n",
       "      <td>Бугров держал Лизу за талию и говорил ей Милая...</td>\n",
       "      <td>бугров держать лиза талия говорить милый делат...</td>\n",
       "      <td>6_NOUN 6_VERB 4_NOUN 5_NOUN 7_VERB 5_ADJF 6_IN...</td>\n",
       "      <td>Бугров держал Лизу за талию и говорил ей : — М...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17734</th>\n",
       "      <td>Чехов</td>\n",
       "      <td>zhivoi-tovar.epub</td>\n",
       "      <td>— Тебе скучно, Лизочка? — заговорил он после н...</td>\n",
       "      <td>Тебе скучно Лизочка заговорил он после непродо...</td>\n",
       "      <td>скучно лизочко заговорить непродолжительный мо...</td>\n",
       "      <td>6_ADVB 7_NOUN 9_VERB 18_ADJF 8_NOUN 8_VERB 6_A...</td>\n",
       "      <td>— Тебе скучно , Лизочка ? — заговорил он после...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17735</th>\n",
       "      <td>Чехов</td>\n",
       "      <td>zhivoi-tovar.epub</td>\n",
       "      <td>Минут через пять в залу вошел Грохольский, зас...</td>\n",
       "      <td>Минут через пять в залу вошел Грохольский засп...</td>\n",
       "      <td>минута пять зала войти грохольский заспанный н...</td>\n",
       "      <td>5_NOUN 4_NUMR 4_NOUN 5_VERB 11_ADJF 9_ADJF 9_A...</td>\n",
       "      <td>Минут через пять в залу вошел Грохольский , за...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17736 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               book  \\\n",
       "0      Аверченко  Averchenko_A-T-Averchenko-Sobranie-sochineniy-...   \n",
       "1      Аверченко  Averchenko_A-T-Averchenko-Sobranie-sochineniy-...   \n",
       "2      Аверченко  Averchenko_A-T-Averchenko-Sobranie-sochineniy-...   \n",
       "3      Аверченко  Averchenko_A-T-Averchenko-Sobranie-sochineniy-...   \n",
       "4      Аверченко  Averchenko_A-T-Averchenko-Sobranie-sochineniy-...   \n",
       "...          ...                                                ...   \n",
       "17731      Чехов                                  zhivoi-tovar.epub   \n",
       "17732      Чехов                                  zhivoi-tovar.epub   \n",
       "17733      Чехов                                  zhivoi-tovar.epub   \n",
       "17734      Чехов                                  zhivoi-tovar.epub   \n",
       "17735      Чехов                                  zhivoi-tovar.epub   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Мы за пять лет. Материалы [к биографии]\\nКак б...   \n",
       "1      Сидел он за большим письменным столом перед де...   \n",
       "2      — Да уж, — качал головой сдержанный Ре-ми. — Н...   \n",
       "3      Поверит ли кто-нибудь, что нами за эти пять ле...   \n",
       "4      8\\nАверченко А. Избранные рассказы. М., 1985. ...   \n",
       "...                                                  ...   \n",
       "17731  — Это ужасно, Григорий Васильич! — заговорил Б...   \n",
       "17732  Грохольский жил всё в той же даче… Надежды и ж...   \n",
       "17733  Бугров держал Лизу за талию и говорил ей:\\n— М...   \n",
       "17734  — Тебе скучно, Лизочка? — заговорил он после н...   \n",
       "17735  Минут через пять в залу вошел Грохольский, зас...   \n",
       "\n",
       "                                           text_no_punkt  \\\n",
       "0      Мы за пять лет Материалы к биографии Как будто...   \n",
       "1      Сидел он за большим письменным столом перед де...   \n",
       "2      Да уж качал головой сдержанный Ре-ми Нехорошо ...   \n",
       "3      Поверит ли кто-нибудь что нами за эти пять лет...   \n",
       "4      8 Аверченко А Избранные рассказы М 1985 С 7 9 ...   \n",
       "...                                                  ...   \n",
       "17731  Это ужасно Григорий Васильич заговорил Бугров ...   \n",
       "17732  Грохольский жил всё в той же даче Надежды и же...   \n",
       "17733  Бугров держал Лизу за талию и говорил ей Милая...   \n",
       "17734  Тебе скучно Лизочка заговорил он после непродо...   \n",
       "17735  Минут через пять в залу вошел Грохольский засп...   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "0      пять год материал биография кроваво-красный ра...   \n",
       "1      сидеть больший письменный стол деревянный доск...   \n",
       "2      качать голова сдержать ре-ми нехорошо нехорошо...   \n",
       "3      поверить кто-нибудь пять год совместно м.г кор...   \n",
       "4      8 избранный рассказ м 1985 7 9 ежегодник 156 1...   \n",
       "...                                                  ...   \n",
       "17731  это ужасно григорий васильич заговорить бугров...   \n",
       "17732  грохольский жить всё дача надежда желание мале...   \n",
       "17733  бугров держать лиза талия говорить милый делат...   \n",
       "17734  скучно лизочко заговорить непродолжительный мо...   \n",
       "17735  минута пять зала войти грохольский заспанный н...   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      4_NUMR 3_NOUN 9_NOUN 9_NOUN 15_ADJF 6_NOUN 8_V...   \n",
       "1      5_VERB 7_ADJF 10_ADJF 6_NOUN 10_ADJF 6_NOUN 6_...   \n",
       "2      5_VERB 7_NOUN 10_PRTF 5_None 8_ADVB 8_ADVB 4_A...   \n",
       "3      7_VERB 10_NPRO 4_NUMR 3_NOUN 9_ADVB 3_None 11_...   \n",
       "4      1_None 9_ADJF 8_NOUN 1_NOUN 4_None 1_None 1_No...   \n",
       "...                                                  ...   \n",
       "17731  3_PRCL 6_ADVB 8_NOUN 8_NOUN 9_VERB 6_NOUN 6_GR...   \n",
       "17732  11_ADJF 3_VERB 3_PRCL 4_NOUN 7_NOUN 7_NOUN 9_A...   \n",
       "17733  6_NOUN 6_VERB 4_NOUN 5_NOUN 7_VERB 5_ADJF 6_IN...   \n",
       "17734  6_ADVB 7_NOUN 9_VERB 18_ADJF 8_NOUN 8_VERB 6_A...   \n",
       "17735  5_NOUN 4_NUMR 4_NOUN 5_VERB 11_ADJF 9_ADJF 9_A...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      Мы за пять лет . Материалы [ к биографии ] Как...  \n",
       "1      Сидел он за большим письменным столом перед де...  \n",
       "2      — Да уж , — качал головой сдержанный Ре-ми . —...  \n",
       "3      Поверит ли кто-нибудь , что нами за эти пять л...  \n",
       "4      8 Аверченко А. Избранные рассказы . М. , 1985 ...  \n",
       "...                                                  ...  \n",
       "17731  — Это ужасно , Григорий Васильич ! — заговорил...  \n",
       "17732  Грохольский жил всё в той же даче… Надежды и ж...  \n",
       "17733  Бугров держал Лизу за талию и говорил ей : — М...  \n",
       "17734  — Тебе скучно , Лизочка ? — заговорил он после...  \n",
       "17735  Минут через пять в залу вошел Грохольский , за...  \n",
       "\n",
       "[17736 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9457f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def make_words(sentences):\n",
    "    words = list(chain(*(nltk.word_tokenize(sentence) for sentence in sentences)))\n",
    "    return words\n",
    "\n",
    "def word_len_avg(words):\n",
    "    total_len = sum(len(word) for word in words)\n",
    "    return total_len / len(words)\n",
    "\n",
    "def word_per_sentence(sentences, words):\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    return np.log(num_words / num_sentences)\n",
    "\n",
    "def exclamation_density(sentences):\n",
    "    exclamations_cnt = len([s for s in sentences if '!' in s])\n",
    "    return exclamations_cnt / len(sentences)\n",
    "\n",
    "def question_density(sentences):\n",
    "    questions_cnt = len([s for s in sentences if '?' in s])\n",
    "    return questions_cnt / len(sentences)\n",
    "\n",
    "def comma_density(sentences):\n",
    "    questions_cnt = sum([s.count(\",\") for s in sentences])\n",
    "    return questions_cnt / len(sentences)\n",
    "\n",
    "def dialogue_density(sentences):\n",
    "    long_dash = chr(8212)\n",
    "    counts = len([sentence for sentence in sentences if sentence.startswith(long_dash)])\n",
    "    return counts / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dde1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(df):\n",
    "    tokenized_text = df.text.map(split_sentences)\n",
    "    words = tokenized_text.map(make_words)\n",
    "    temp_df = pd.DataFrame({\"words\": words, \"sentences\":tokenized_text})\n",
    "    feature_map = {\n",
    "        \"word_avg_length\": words.map(word_len_avg),\n",
    "        \"words_per_sentence\": temp_df.apply(lambda x: word_per_sentence(x[\"sentences\"], x[\"words\"]),axis=1),\n",
    "        \"exclamation_density\": tokenized_text.map(exclamation_density),\n",
    "        \"question_density\": tokenized_text.map(question_density),\n",
    "        \"comma_density\":  tokenized_text.map(question_density),\n",
    "        \"dialogue_density\": tokenized_text.map(dialogue_density),\n",
    "    }\n",
    "    feature_df = pd.DataFrame(feature_map)\n",
    "    \n",
    "    return pd.merge(df, feature_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f863e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = count_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613d0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counts'] = df.book.map(df.book.value_counts())\n",
    "def add_class_based_weigths(df):\n",
    "    author_counts= df.author.map(df.author.value_counts())\n",
    "    num_authors = len(df.author.unique())\n",
    "    df[\"probs\"] = 1/(author_counts*num_authors)\n",
    "add_class_based_weigths(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178629af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample(dataframe, size=0.1):\n",
    "    df_size = len(dataframe)\n",
    "    idx = np.random.choice(df_size,size=int(df_size * size), p=dataframe.probs)\n",
    "    return dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce32db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, share=0.5, seed=10):\n",
    "    rg = np.random.default_rng(seed)\n",
    "    labels = df.author.unique()\n",
    "    shuffled_labels = labels[rg.permutation(len(labels))]\n",
    "    \n",
    "    train_label = []\n",
    "    test_label = []\n",
    "    train_counts = 0\n",
    "    test_counts = 0\n",
    "\n",
    "    for label in shuffled_labels:\n",
    "        df_slice = df[df.author==label][[\"book\", \"counts\"]]\n",
    "        unique_books = df_slice.drop_duplicates(\"book\")\n",
    "        if len(unique_books) < 2:\n",
    "            raise ValueError(f\"too few books of author: {label}\")\n",
    "        permunation = rg.permutation(len(unique_books))\n",
    "        shuffled_books = unique_books.iloc[permunation]\n",
    "        train_labels = []\n",
    "        test_labels = []\n",
    "        label_train_count = 0\n",
    "        label_total_count = np.sum(shuffled_books[\"counts\"])\n",
    "        i = 0\n",
    "        for _, row in shuffled_books.iloc[:-1].iterrows():\n",
    "            num_segments = row.counts\n",
    "            if not label_train_count:\n",
    "                i+=1\n",
    "                label_train_count += num_segments\n",
    "                continue\n",
    "            new_train_count = label_train_count + num_segments\n",
    "            new_share = new_train_count / label_total_count\n",
    "            old_share = label_train_count / label_total_count\n",
    "            if abs(share - new_share) > abs(share - old_share):\n",
    "                break\n",
    "            i+=1\n",
    "            label_train_count += num_segments\n",
    "        train_label.extend(shuffled_books.iloc[:i].book)\n",
    "        test_label.extend(shuffled_books.iloc[i:].book)\n",
    "        train_counts += label_train_count\n",
    "        test_counts += label_total_count - label_train_count\n",
    "    train_df = df[df[\"book\"].isin(set(train_label))]\n",
    "    test_df = df[df[\"book\"].isin(set(test_label))]\n",
    "    assert len(train_df) + len(test_df) == len(df), \\\n",
    "        \"split is wrong\"\n",
    "    y_train = train_df[\"author\"]\n",
    "    y_test = test_df[\"author\"]\n",
    "    return train_df, test_df, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d460d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vectorizer(frame, n_min=1, n=2, max_count=10000, column=\"lemmas\", stops=None):\n",
    "    texts_vector = frame[column]\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n), max_features=max_count, norm='l2', stop_words=stops)\n",
    "    return vectorizer\n",
    "\n",
    "def get_author_vectorizer(frame, n_min=1, n=2, max_count=10000, column=\"lemmas\", stops=None):\n",
    "    grouped_text = frame.groupby(\"author\", as_index = False)[column].agg({column: ' '.join})\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n), max_features=max_count, norm='l2', stop_words=stops)\n",
    "    texts_vector = frame[column]\n",
    "    vectorizer.fit(grouped_text[column])\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3d281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(frame, column=\"author\"):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(frame[column])\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0e423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(function, x_start, learning_rate, epsilon, delta, num_iterations):\n",
    "    x_curr = x_start\n",
    "    df_x = sp.diff(function) # производная\n",
    "\n",
    "    trace = []\n",
    "    trace.append(x_curr)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        derivative = df_x.subs(_x, x_curr)\n",
    "        if abs(derivative) < delta:\n",
    "            print(f\"derivative smaller that threshold {delta}\",file=sys.stderr)\n",
    "            break\n",
    "        x_new = x_curr - learning_rate * derivative\n",
    "        trace.append(x_new)\n",
    "\n",
    "        if abs(x_new - x_curr) < epsilon:\n",
    "            print(\"update speed is less than threshold {epsilon}\", file=sys.stderr)\n",
    "            break\n",
    "\n",
    "        x_curr = x_new\n",
    "\n",
    "    return x_curr, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d9e202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seq(val):\n",
    "    iterable = hasattr(val, \"__iter__\")\n",
    "    not_string = not isinstance(val , str)\n",
    "    return iterable and not_string\n",
    "\n",
    "class Featurebuilder:\n",
    "    feature_mapping = {\"tokens\": \"vectorizer\", \n",
    "                       \"text_no_punkt\": \"vectorizer\",\n",
    "                       \"lemmas\": \"vectorizer\",\n",
    "                       \"tags\": \"vectorizer\"}\n",
    "    \n",
    "    def __init__(self, *args, **vectorizers):\n",
    "        vectorizers = {k.lstrip(\"vec_\"):v for k,v in vectorizers.items()}\n",
    "        featurelist = self.pack_features(args)\n",
    "        self.vectorizers = {}\n",
    "        for feature in featurelist:\n",
    "            processor = self.feature_mapping.get(feature, None)\n",
    "            if not processor:\n",
    "                self.vectorizers[feature] = None\n",
    "                continue\n",
    "            if processor != \"vectorizer\":\n",
    "                raise ValueError(\"only vectorizing is supported for non-scalar features\")\n",
    "            vectorizer = vectorizers.get(feature, None)\n",
    "            if vectorizer is None:\n",
    "                raise ValueError(f\"no vectorizer for feature: {feature}\")\n",
    "            self.vectorizers[feature] = vectorizer\n",
    "        self.ordered_ft = list(sorted(featurelist,\n",
    "                                      key=lambda x: self.feature_mapping.get(x, \"\")))\n",
    "        self.ordered_proc = [self.feature_mapping.get(ft, None) for ft in self.ordered_ft]\n",
    "        self._initialized = False\n",
    "        self.feature_idx = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def pack_features(features):\n",
    "        attrs = (ft if check_seq(ft)  else (ft, ) for ft in features)\n",
    "        return list(chain(*attrs))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_last_occurence(seq, val):\n",
    "        return len(seq) - 1 - seq[::-1].index(val)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_first_occurence(seq, val):\n",
    "        return seq.index(val)\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        feature_positions = []\n",
    "        feature_matrices = []\n",
    "        for proc in set(self.ordered_proc):\n",
    "            first_idx  = self.get_first_occurence(self.ordered_proc, proc)\n",
    "            last_idx = self.get_last_occurence(self.ordered_proc, proc)\n",
    "            feature_slice = self.ordered_ft[first_idx:last_idx+1]\n",
    "            \n",
    "            smat, fpos = self.bulk_process(df, proc, feature_slice)\n",
    "            \n",
    "            feature_matrices.append(smat)\n",
    "            feature_positions.extend(fpos)\n",
    "            \n",
    "        final_matrix = sp.sparse.hstack(feature_matrices)\n",
    "        counter = 0\n",
    "        self.feature_idx = dict()\n",
    "        for ft, length in zip(self.ordered_ft, feature_positions):\n",
    "            self.feature_idx[ft] = (counter, length)\n",
    "            counter+=length\n",
    "        self._initialized = True\n",
    "        return final_matrix\n",
    "    \n",
    "    def transform(self, df):\n",
    "        feature_matrices = []\n",
    "        for proc in set(self.ordered_proc):\n",
    "            first_idx  = self.get_first_occurence(self.ordered_proc, proc)\n",
    "            last_idx = self.get_last_occurence(self.ordered_proc, proc)\n",
    "            feature_slice = self.ordered_ft[first_idx:last_idx+1]\n",
    "            smat, fpos = self.bulk_process(df, proc, feature_slice)\n",
    "            feature_matrices.append(smat)\n",
    "        final_matrix = sp.sparse.hstack(feature_matrices)\n",
    "        return final_matrix\n",
    "    \n",
    "    def bulk_process(self, df, proc, featurelist):\n",
    "        if proc is None:\n",
    "            columns = df[featurelist].to_numpy(dtype=np.float64)\n",
    "            mat = sp.sparse.csr_matrix(columns)\n",
    "            indices = [1 for ft in featurelist]\n",
    "            return mat, indices\n",
    "        elif proc != \"vectorizer\":\n",
    "            raise ValueError(\"only vectorizers supported now\")\n",
    "        else:\n",
    "            matrices = []\n",
    "            indices = []\n",
    "            for i, feature in enumerate(featurelist):\n",
    "                vectorizer = self.vectorizers[feature]\n",
    "                mat = vectorizer.transform(df[feature])\n",
    "                matrices.append(mat)\n",
    "                indices.append(mat.shape[1])\n",
    "            return sp.sparse.hstack(matrices), indices\n",
    "    \n",
    "    \n",
    "    def find_idx(self, idx):\n",
    "        for key, (start, length) in self.feature_idx.items():\n",
    "            if  start <= idx < (start + length):\n",
    "                break\n",
    "        else:\n",
    "                raise ValueError(f\"index {idx} too big\")\n",
    "        vectorizer = self.vectorizers[key]\n",
    "        if vectorizer is None:\n",
    "            return key\n",
    "        else:\n",
    "            features = vectorizer.get_feature_names_out()\n",
    "            return features[idx-start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f694d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(label_enc, data_enc, clf, n):\n",
    "    names = label_enc.classes_\n",
    "    coeffs = clf.coef_\n",
    "    author_dict = dict()\n",
    "    for i, author in enumerate(names):\n",
    "        args_sorted = list(reversed(np.argsort(coeffs[i])[-n:]))\n",
    "        features = [data_enc.find_idx(idx) for idx in args_sorted]\n",
    "        author_dict[author] = features\n",
    "    df = pd.DataFrame(author_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32e4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crossval_twofold(frame, clf, *args, split=0.5, vectorizer_dict=None, avg=\"micro\"):\n",
    "    split = train_test_split(df, share=0.5)\n",
    "    x_split = split[:2]\n",
    "    y_split = split[2:]\n",
    "    scores = []\n",
    "    for train_idx, test_idx in ((1, 0), (0, 1)):\n",
    "        df_train = x_split[train_idx]\n",
    "        df_test = x_split[test_idx]\n",
    "        \n",
    "        y_train = y_split[train_idx]\n",
    "        y_test = y_split[test_idx]\n",
    "        \n",
    "        if vectorizer_dict == None:\n",
    "            raise ValueError(\"not using any vectorizer!\")\n",
    "        vecs = dict()\n",
    "        for fname, params in vectorizer_dict.items():\n",
    "            vecs[\"vec_\"+fname] = get_author_vectorizer(df_train, **params, column=fname)\n",
    "            \n",
    "        data_encoder = Featurebuilder(*args, **vecs)\n",
    "        x_train = data_encoder.fit_transform(df_train)\n",
    "        x_test = data_encoder.fit_transform(df_test)\n",
    "        \n",
    "        label_encoder = get_encoder(frame=df)\n",
    "        y_train = label_encoder.transform(y_train)\n",
    "        y_test = label_encoder.transform(y_test)\n",
    "        \n",
    "        clf.fit(x_train, y_train)\n",
    "        score = f1_score(clf.predict(x_test), y_test, average=avg)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f4ad7",
   "metadata": {},
   "source": [
    "### Попробуем простую регрессию на лемматизированном тексте:\n",
    "возьмем лемматизированные отрезки текста, векторизируем их и попробуем предсказать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6620532b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6997042915120277\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\",\n",
    "                         warm_start=False,\n",
    "                         max_iter=500)\n",
    "param_dict = {\n",
    "    \"lemmas\": {\n",
    "        \"max_count\": 10000,\n",
    "    }\n",
    "}\n",
    "scores = train_crossval_twofold(df, clf, \"lemmas\", vectorizer_dict=param_dict)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf1865",
   "metadata": {},
   "source": [
    "### Попробуем прогнать регрессию на комбинациях частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "47d9b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48267047031434407\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\",\n",
    "                         warm_start=False,\n",
    "                         max_iter=500)\n",
    "param_dict = {\n",
    "    \"tags\": {\n",
    "        \"max_count\": 10000,\n",
    "        \"n_min\":1,\n",
    "        \"n\":3\n",
    "    }\n",
    "}\n",
    "scores = train_crossval_twofold(df, clf, \"tags\", vectorizer_dict=param_dict)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefcee9",
   "metadata": {},
   "source": [
    "Результат слабый. Мы убираем большую часть информации из текста, оставляя только зависимости, но гибкости модели (или информации) недостаточно для эффективного разделения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0551ad3",
   "metadata": {},
   "source": [
    "### Посмотрим на то,помогут ли токены вместе с пунктуацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e90b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6634488587805086\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\",\n",
    "                         warm_start=False,\n",
    "                         max_iter=500)\n",
    "param_dict = {\n",
    "    \"tokens\": {\n",
    "        \"max_count\": 10000,\n",
    "        \"n_min\":1,\n",
    "        \"n\":3\n",
    "    }\n",
    "}\n",
    "scores = train_crossval_twofold(df, clf, \"tokens\", vectorizer_dict=param_dict)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dc775",
   "metadata": {},
   "source": [
    "Результат хуже, чем просто на леммах. попробуем комбинировать признаки- добавим к леммам н-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a4a6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7277471517678047\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\",\n",
    "                         warm_start=False,\n",
    "                         max_iter=500)\n",
    "param_dict = {\n",
    "    \"tokens\": {\n",
    "        \"max_count\": 10000,\n",
    "        \"n_min\":1,\n",
    "        \"n\":3,},\n",
    "    \"lemmas\": {\n",
    "        \"max_count\": 10000,\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = train_crossval_twofold(df, clf, \"tokens\", \"lemmas\", vectorizer_dict=param_dict)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7864c",
   "metadata": {},
   "source": [
    "### Лучший вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6482fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_features = [\"word_avg_length\", \"words_per_sentence\",\n",
    "                   \"exclamation_density\", \"question_density\",\n",
    "                   \"comma_density\", \"dialogue_density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a4b7539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dm12h/py37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/dm12h/py37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885212807559792\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\",\n",
    "                         warm_start=False,\n",
    "                         max_iter=500)\n",
    "param_dict = {\n",
    "    \"tags\": {\n",
    "        \"max_count\": 5000,\n",
    "        \"n_min\":1,\n",
    "        \"n\":3,},\n",
    "    \"lemmas\": {\n",
    "        \"max_count\": 10000,\n",
    "    }\n",
    "}\n",
    "\n",
    "scores = train_crossval_twofold(df, clf, \"tags\", \"lemmas\", scalar_features, vectorizer_dict=param_dict)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27496fe",
   "metadata": {},
   "source": [
    "Чтобы визуализировать модель, мы не пользуемся кросс-валидацией, а берем обычное разбиение.\n",
    "Добавим к частям речи и тегам рассчитанные ранее скалярные параметры. они несущественно увеличивают число признаков, но могут быть полезными, так как вектор слов может не покрывать отдельные отрезки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f0486",
   "metadata": {},
   "source": [
    "Также при подборе параметров обнаружилось, что дефолтная регуляризация слишком жесткая, и при меньших значениях модель сходится лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711d5de",
   "metadata": {},
   "source": [
    "Стопслова позволяют, хоть и не полностью, убрать из текста упоминания писателей, попадающиеся из-за грязных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1b37970",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_remove = df[\"author\"].unique()\n",
    "stopwords = [name.lower() for name in names_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dc70fa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['аверченко',\n",
       " 'булгаков',\n",
       " 'бунин',\n",
       " 'гоголь',\n",
       " 'горький',\n",
       " 'достоевский',\n",
       " 'карамзин',\n",
       " 'куприн',\n",
       " 'лермонтов',\n",
       " 'мордовцев',\n",
       " 'пушкин',\n",
       " 'салтыков-щедрин',\n",
       " 'толстой',\n",
       " 'тургенев',\n",
       " 'чехов']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e15930c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = get_encoder(df)\n",
    "df_train, df_test, y_train, y_test = train_test_split(df, share=0.5)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5e3a70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dm12h/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['салтыков', 'щедрин'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "token_vectorizer = get_author_vectorizer(df_train, max_count=10000, column=\"lemmas\", stops=stopwords, n_min=1, n=3)\n",
    "tag_vectorizer = get_author_vectorizer(df_train, max_count=10000, column=\"tags\", stops=stopwords, n_min=1, n=3)\n",
    "\n",
    "data_encoder = Featurebuilder(\"tokens\",\n",
    "                              \"tags\",\n",
    "                              scalar_features,\n",
    "                              vec_tokens=token_vectorizer,\n",
    "                              vec_tags=tag_vectorizer)\n",
    "x_train = data_encoder.fit_transform(df_train)\n",
    "x_test = data_encoder.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f7049b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dm12h/py37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5000, class_weight='balanced', max_iter=1000,\n",
       "                   random_state=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"l2\",\n",
    "                         random_state=10,\n",
    "                         C=5000,\n",
    "                         class_weight=\"balanced\",\n",
    "                         max_iter=1000)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "affcf483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218957108915843"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(clf.predict(x_test), y_test, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e85db2",
   "metadata": {},
   "source": [
    "Посмотрим на наиболее весомые фичи для каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5dfaab4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Аверченко</th>\n",
       "      <th>Булгаков</th>\n",
       "      <th>Бунин</th>\n",
       "      <th>Гоголь</th>\n",
       "      <th>Горький</th>\n",
       "      <th>Достоевский</th>\n",
       "      <th>Карамзин</th>\n",
       "      <th>Куприн</th>\n",
       "      <th>Лермонтов</th>\n",
       "      <th>Мордовцев</th>\n",
       "      <th>Пушкин</th>\n",
       "      <th>Салтыков-Щедрин</th>\n",
       "      <th>Толстой</th>\n",
       "      <th>Тургенев</th>\n",
       "      <th>Чехов</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>сандерс</td>\n",
       "      <td>михаил</td>\n",
       "      <td>возле</td>\n",
       "      <td>чичиков</td>\n",
       "      <td>самгин</td>\n",
       "      <td>всё</td>\n",
       "      <td>наталья</td>\n",
       "      <td>точно</td>\n",
       "      <td>грушницкий</td>\n",
       "      <td>2_none</td>\n",
       "      <td>дубровский</td>\n",
       "      <td>словно</td>\n",
       "      <td>левин</td>\n",
       "      <td>лаврецкий</td>\n",
       "      <td>всё</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>однажды</td>\n",
       "      <td>сталин</td>\n",
       "      <td>4_none</td>\n",
       "      <td>голова</td>\n",
       "      <td>клим</td>\n",
       "      <td>раскольников</td>\n",
       "      <td>лиза</td>\n",
       "      <td>ромашов</td>\n",
       "      <td>мы</td>\n",
       "      <td>словно</td>\n",
       "      <td>несколько</td>\n",
       "      <td>6_conj</td>\n",
       "      <td>пьер</td>\n",
       "      <td>рудин</td>\n",
       "      <td>урбенин</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>подходцев</td>\n",
       "      <td>маргарита</td>\n",
       "      <td>каменский</td>\n",
       "      <td>нибудь</td>\n",
       "      <td>артамонов</td>\n",
       "      <td>наконец</td>\n",
       "      <td>5_adjf</td>\n",
       "      <td>царь</td>\n",
       "      <td>слепой</td>\n",
       "      <td>dialogue_density</td>\n",
       "      <td>савельич</td>\n",
       "      <td>однако</td>\n",
       "      <td>альберт</td>\n",
       "      <td>герасим</td>\n",
       "      <td>грохольский</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ещё</td>\n",
       "      <td>гудок</td>\n",
       "      <td>турбина</td>\n",
       "      <td>вовсе</td>\n",
       "      <td>пётр</td>\n",
       "      <td>тотчас</td>\n",
       "      <td>4_none</td>\n",
       "      <td>дружинин</td>\n",
       "      <td>печорин</td>\n",
       "      <td>девушка</td>\n",
       "      <td>крепость</td>\n",
       "      <td>арина</td>\n",
       "      <td>вронский</td>\n",
       "      <td>лежнев</td>\n",
       "      <td>ольга</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>нужно</td>\n",
       "      <td>1924</td>\n",
       "      <td>8_advb</td>\n",
       "      <td>1_none</td>\n",
       "      <td>всё</td>\n",
       "      <td>1_noun</td>\n",
       "      <td>боярин</td>\n",
       "      <td>соломон</td>\n",
       "      <td>франц</td>\n",
       "      <td>лаодика</td>\n",
       "      <td>её</td>\n",
       "      <td>салтыков</td>\n",
       "      <td>дутлов</td>\n",
       "      <td>дарья</td>\n",
       "      <td>следователь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>мать</td>\n",
       "      <td>прокуратор</td>\n",
       "      <td>опять</td>\n",
       "      <td>наконец</td>\n",
       "      <td>яков</td>\n",
       "      <td>версилов</td>\n",
       "      <td>эраст</td>\n",
       "      <td>сергей</td>\n",
       "      <td>один</td>\n",
       "      <td>сперанский</td>\n",
       "      <td>швабрин</td>\n",
       "      <td>щедрина</td>\n",
       "      <td>3_none</td>\n",
       "      <td>5_noun 10_noun</td>\n",
       "      <td>граф</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12_adjf</td>\n",
       "      <td>нету</td>\n",
       "      <td>яков петрович</td>\n",
       "      <td>шиллер</td>\n",
       "      <td>её</td>\n",
       "      <td>арестант</td>\n",
       "      <td>6_adjf</td>\n",
       "      <td>мистер</td>\n",
       "      <td>штабс</td>\n",
       "      <td>4_none</td>\n",
       "      <td>батюшка</td>\n",
       "      <td>владимирыч</td>\n",
       "      <td>анна</td>\n",
       "      <td>паншин</td>\n",
       "      <td>графа</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>троцкий</td>\n",
       "      <td>профессор</td>\n",
       "      <td>например</td>\n",
       "      <td>3_prcl</td>\n",
       "      <td>точно</td>\n",
       "      <td>фома</td>\n",
       "      <td>exclamation_density</td>\n",
       "      <td>дедушка</td>\n",
       "      <td>вы</td>\n",
       "      <td>бурцев</td>\n",
       "      <td>смотритель</td>\n",
       "      <td>аннинька</td>\n",
       "      <td>8_grnd</td>\n",
       "      <td>александра</td>\n",
       "      <td>оленька</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4_npro</td>\n",
       "      <td>совершенно</td>\n",
       "      <td>3_prcl</td>\n",
       "      <td>нужно</td>\n",
       "      <td>социализм</td>\n",
       "      <td>князь</td>\n",
       "      <td>сердце</td>\n",
       "      <td>также</td>\n",
       "      <td>капитан</td>\n",
       "      <td>амвросий</td>\n",
       "      <td>ибрагим</td>\n",
       "      <td>покуда</td>\n",
       "      <td>ежели</td>\n",
       "      <td>волынцев</td>\n",
       "      <td>когда</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>снова</td>\n",
       "      <td>1922</td>\n",
       "      <td>медленно</td>\n",
       "      <td>никак</td>\n",
       "      <td>народ</td>\n",
       "      <td>свидригайлов</td>\n",
       "      <td>матвей</td>\n",
       "      <td>вера</td>\n",
       "      <td>казак</td>\n",
       "      <td>папа</td>\n",
       "      <td>германн</td>\n",
       "      <td>иудушка</td>\n",
       "      <td>наполеон</td>\n",
       "      <td>гаврила</td>\n",
       "      <td>всё это</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>когда</td>\n",
       "      <td>коровьев</td>\n",
       "      <td>порою</td>\n",
       "      <td>копейкин</td>\n",
       "      <td>человек</td>\n",
       "      <td>чрезвычайно</td>\n",
       "      <td>старушка</td>\n",
       "      <td>бузыга</td>\n",
       "      <td>5_advb</td>\n",
       "      <td>вон</td>\n",
       "      <td>маша</td>\n",
       "      <td>глупов</td>\n",
       "      <td>офицер</td>\n",
       "      <td>10_noun</td>\n",
       "      <td>петрович</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>поэтому</td>\n",
       "      <td>лишь</td>\n",
       "      <td>сперва</td>\n",
       "      <td>акакий</td>\n",
       "      <td>пред</td>\n",
       "      <td>4_none 1_noun</td>\n",
       "      <td>но</td>\n",
       "      <td>поручик</td>\n",
       "      <td>нынче</td>\n",
       "      <td>это</td>\n",
       "      <td>лиза</td>\n",
       "      <td>щедрин</td>\n",
       "      <td>евгений</td>\n",
       "      <td>8_verb</td>\n",
       "      <td>dialogue_density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>около</td>\n",
       "      <td>голый</td>\n",
       "      <td>аверкий</td>\n",
       "      <td>акакиевич</td>\n",
       "      <td>никита</td>\n",
       "      <td>1860</td>\n",
       "      <td>ах</td>\n",
       "      <td>кашинцев</td>\n",
       "      <td>что</td>\n",
       "      <td>наполеон</td>\n",
       "      <td>настя</td>\n",
       "      <td>летописец</td>\n",
       "      <td>9_grnd</td>\n",
       "      <td>марфа</td>\n",
       "      <td>камышева</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1_none</td>\n",
       "      <td>почему</td>\n",
       "      <td>вечер</td>\n",
       "      <td>акакий акакиевич</td>\n",
       "      <td>снова</td>\n",
       "      <td>давеча</td>\n",
       "      <td>алексей</td>\n",
       "      <td>суламифь</td>\n",
       "      <td>доктор</td>\n",
       "      <td>монтуемтауи</td>\n",
       "      <td>оренбург</td>\n",
       "      <td>сударь</td>\n",
       "      <td>акулина</td>\n",
       "      <td>10_verb</td>\n",
       "      <td>пастух</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>громов</td>\n",
       "      <td>берлиоз</td>\n",
       "      <td>dialogue_density</td>\n",
       "      <td>невский проспект</td>\n",
       "      <td>интеллигенция</td>\n",
       "      <td>ламберт</td>\n",
       "      <td>дочь</td>\n",
       "      <td>рита</td>\n",
       "      <td>question_density</td>\n",
       "      <td>матушка</td>\n",
       "      <td>гаврила</td>\n",
       "      <td>бригадир</td>\n",
       "      <td>граф</td>\n",
       "      <td>наталья</td>\n",
       "      <td>дмитрий</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Аверченко    Булгаков             Бунин            Гоголь        Горький  \\\n",
       "0     сандерс      михаил             возле           чичиков         самгин   \n",
       "1     однажды      сталин            4_none            голова           клим   \n",
       "2   подходцев   маргарита         каменский            нибудь      артамонов   \n",
       "3         ещё       гудок           турбина             вовсе           пётр   \n",
       "4       нужно        1924            8_advb            1_none            всё   \n",
       "5        мать  прокуратор             опять           наконец           яков   \n",
       "6     12_adjf        нету     яков петрович            шиллер             её   \n",
       "7     троцкий   профессор          например            3_prcl          точно   \n",
       "8      4_npro  совершенно            3_prcl             нужно      социализм   \n",
       "9       снова        1922          медленно             никак          народ   \n",
       "10      когда    коровьев             порою          копейкин        человек   \n",
       "11    поэтому        лишь            сперва            акакий           пред   \n",
       "12      около       голый           аверкий         акакиевич         никита   \n",
       "13     1_none      почему             вечер  акакий акакиевич          снова   \n",
       "14     громов     берлиоз  dialogue_density  невский проспект  интеллигенция   \n",
       "\n",
       "      Достоевский             Карамзин    Куприн         Лермонтов  \\\n",
       "0             всё              наталья     точно        грушницкий   \n",
       "1    раскольников                 лиза   ромашов                мы   \n",
       "2         наконец               5_adjf      царь            слепой   \n",
       "3          тотчас               4_none  дружинин           печорин   \n",
       "4          1_noun               боярин   соломон             франц   \n",
       "5        версилов                эраст    сергей              один   \n",
       "6        арестант               6_adjf    мистер             штабс   \n",
       "7            фома  exclamation_density   дедушка                вы   \n",
       "8           князь               сердце     также           капитан   \n",
       "9    свидригайлов               матвей      вера             казак   \n",
       "10    чрезвычайно             старушка    бузыга            5_advb   \n",
       "11  4_none 1_noun                   но   поручик             нынче   \n",
       "12           1860                   ах  кашинцев               что   \n",
       "13         давеча              алексей  суламифь            доктор   \n",
       "14        ламберт                 дочь      рита  question_density   \n",
       "\n",
       "           Мордовцев      Пушкин Салтыков-Щедрин   Толстой        Тургенев  \\\n",
       "0             2_none  дубровский          словно     левин       лаврецкий   \n",
       "1             словно   несколько          6_conj      пьер           рудин   \n",
       "2   dialogue_density    савельич          однако   альберт         герасим   \n",
       "3            девушка    крепость           арина  вронский          лежнев   \n",
       "4            лаодика          её        салтыков    дутлов           дарья   \n",
       "5         сперанский     швабрин         щедрина    3_none  5_noun 10_noun   \n",
       "6             4_none     батюшка      владимирыч      анна          паншин   \n",
       "7             бурцев  смотритель        аннинька    8_grnd      александра   \n",
       "8           амвросий     ибрагим          покуда     ежели        волынцев   \n",
       "9               папа     германн         иудушка  наполеон         гаврила   \n",
       "10               вон        маша          глупов    офицер         10_noun   \n",
       "11               это        лиза          щедрин   евгений          8_verb   \n",
       "12          наполеон       настя       летописец    9_grnd           марфа   \n",
       "13       монтуемтауи    оренбург          сударь   акулина         10_verb   \n",
       "14           матушка     гаврила        бригадир      граф         наталья   \n",
       "\n",
       "               Чехов  \n",
       "0                всё  \n",
       "1            урбенин  \n",
       "2        грохольский  \n",
       "3              ольга  \n",
       "4        следователь  \n",
       "5               граф  \n",
       "6              графа  \n",
       "7            оленька  \n",
       "8              когда  \n",
       "9            всё это  \n",
       "10          петрович  \n",
       "11  dialogue_density  \n",
       "12          камышева  \n",
       "13            пастух  \n",
       "14           дмитрий  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_features(label_encoder, data_encoder, clf, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188ddc0",
   "metadata": {},
   "source": [
    "При обработке датасета осталась нерешенная проблема с маленьким количеством данных для некоторых классов. Необходимо перепроверить обработку и добавить равномерный самплинг для минимизации потерь качества. тем не менее, метрика качества, более взвешенная по классам, выдает результат ощутимо ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cfca4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5581588623118392"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(clf.predict(x_test), y_test, average=\"macro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
